{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlIM29s_He9h",
        "outputId": "4cffdc62-cedd-4c12-9811-0cd323cc23c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K\n",
            "up to date, audited 23 packages in 1s\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K\n",
            "2 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit prophet tensorflow scikit-learn pyngrok plotly pandas numpy faker\n",
        "!npm install -q localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import uuid\n",
        "from faker import Faker\n",
        "from pandas.tseries.offsets import DateOffset\n",
        "\n",
        "fake = Faker()\n",
        "np.random.seed(42)\n",
        "\n",
        "TRANSPORT_EMISSION_FACTORS = {\n",
        "    'Flight': 0.85,    # kg CO2e/km\n",
        "    'Maritime': 0.03,  # kg CO2e/km\n",
        "    'Road': 0.21,     # kg CO2e/km\n",
        "    'Train': 0.06      # kg CO2e/km\n",
        "}\n",
        "\n",
        "PACKAGING_IMPACT = {\n",
        "    'plastic': 1.2,\n",
        "    'paper': 0.9\n",
        "}\n",
        "\n",
        "def generate_dataset(num_entries=50000):\n",
        "    \"\"\"Generate synthetic e-commerce carbon footprint data\"\"\"\n",
        "    data = []\n",
        "    start_date = pd.to_datetime('2023-01-01 00:00:00')\n",
        "\n",
        "    for i in range(num_entries):\n",
        "        transport_mode = np.random.choice(\n",
        "            ['Road', 'Flight', 'Maritime', 'Train'],\n",
        "            p=[0.6, 0.1, 0.2, 0.1]\n",
        "        )\n",
        "\n",
        "        base_distance = {\n",
        "            'Flight': np.random.lognormal(4.5, 0.3),\n",
        "            'Maritime': np.random.lognormal(6.0, 0.4),\n",
        "            'Road': np.random.lognormal(3.0, 0.2),\n",
        "            'Train': np.random.lognormal(5.0, 0.3)\n",
        "        }[transport_mode]\n",
        "\n",
        "        distance = np.clip(base_distance, 10, 10000)\n",
        "        packaging = np.random.choice(['plastic', 'paper'], p=[0.65, 0.35])\n",
        "\n",
        "        base_emission = distance * TRANSPORT_EMISSION_FACTORS[transport_mode]\n",
        "        emission = base_emission * PACKAGING_IMPACT[packaging] * np.random.uniform(0.95, 1.05)\n",
        "\n",
        "        data.append({\n",
        "            'transaction_id': str(uuid.uuid4()),\n",
        "            'product_sku': fake.bothify(text='??-#####', letters='ABCDE'),\n",
        "            'distance_km': round(distance, 2),\n",
        "            'packaging_material': packaging,\n",
        "            'supplier_location': fake.country(),\n",
        "            'carbon_emission_kg': round(emission, 2),\n",
        "            'order_value_usd': round(max(10, np.random.lognormal(3.5, 0.5)), 2),\n",
        "            'transport_mode': transport_mode,\n",
        "            'order_date': start_date + DateOffset(hours=i)\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Generate and save dataset\n",
        "df = generate_dataset(50000)\n",
        "df.to_csv('ecommerce_carbon_dataset.csv', index=False)\n",
        "print(\"✅ Dataset generated successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_FBAqHJHl3r",
        "outputId": "38bc85ec-c658-4e26-aec5-b61970f15d4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset generated successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from prophet import Prophet\n",
        "from prophet.serialize import model_to_json, model_from_json\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "import joblib\n",
        "\n",
        "class CarbonAnalytics:\n",
        "    def __init__(self):\n",
        "        self.df = pd.read_csv('ecommerce_carbon_dataset.csv', parse_dates=['order_date'])\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def train_clusters(self):\n",
        "        \"\"\"Train product clusters for recommendations\"\"\"\n",
        "        features = self.df[['distance_km', 'carbon_emission_kg', 'order_value_usd']]\n",
        "        self.scaler.fit(features)\n",
        "        self.kmeans = KMeans(n_clusters=5, n_init=10)\n",
        "        self.kmeans.fit(self.scaler.transform(features))\n",
        "        joblib.dump((self.scaler, self.kmeans), 'clustering_model.pkl')\n",
        "\n",
        "    def train_prophet(self):\n",
        "        \"\"\"Train and save Prophet model using proper serialization\"\"\"\n",
        "        prophet_df = self.df.rename(columns={\n",
        "            'order_date': 'ds',\n",
        "            'carbon_emission_kg': 'y'\n",
        "        })[['ds', 'y']]\n",
        "\n",
        "        model = Prophet(\n",
        "            yearly_seasonality=True,\n",
        "            weekly_seasonality=True,\n",
        "            daily_seasonality=False\n",
        "        )\n",
        "        model.fit(prophet_df)\n",
        "\n",
        "        # Save using Prophet's native serialization\n",
        "        with open('prophet_model.json', 'w') as f:\n",
        "            f.write(model_to_json(model))\n",
        "\n",
        "    def train_lstm(self):\n",
        "        \"\"\"Train LSTM forecasting model\"\"\"\n",
        "        scaler = MinMaxScaler()\n",
        "        scaled_data = scaler.fit_transform(self.df[['carbon_emission_kg']])\n",
        "\n",
        "        # Create sequences\n",
        "        X, y = [], []\n",
        "        for i in range(len(scaled_data)-24):\n",
        "            X.append(scaled_data[i:i+24])\n",
        "            y.append(scaled_data[i+24])\n",
        "\n",
        "        model = Sequential([\n",
        "            LSTM(64, return_sequences=True, input_shape=(24, 1)),\n",
        "            Dropout(0.2),\n",
        "            LSTM(32),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "        model.fit(np.array(X), np.array(y), epochs=10, batch_size=32, verbose=0)\n",
        "        model.save('lstm_model.h5')\n",
        "        joblib.dump(scaler, 'lstm_scaler.pkl')\n",
        "\n",
        "# Initialize and train models\n",
        "analytics = CarbonAnalytics()\n",
        "analytics.train_clusters()\n",
        "analytics.train_prophet()\n",
        "analytics.train_lstm()\n",
        "print(\"✅ Models trained successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "579D9hFYH287",
        "outputId": "f51f0e34-dfda-4cbb-b871-b3c70baabbaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp8f83fqv7/3hy6d36u.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp8f83fqv7/i1_7z15z.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=29931', 'data', 'file=/tmp/tmp8f83fqv7/3hy6d36u.json', 'init=/tmp/tmp8f83fqv7/i1_7z15z.json', 'output', 'file=/tmp/tmp8f83fqv7/prophet_modelx62ft_u3/prophet_model-20250615201748.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "20:17:48 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "20:17:58 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Models trained successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from prophet.plot import plot_plotly\n",
        "import joblib\n",
        "from prophet.serialize import model_from_json\n",
        "\n",
        "# Configuration\n",
        "st.set_page_config(page_title=\"Carbon Analytics\", layout=\"wide\")\n",
        "st.title(\"🌱 E-commerce Carbon Intelligence\")\n",
        "\n",
        "# Load assets\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    return pd.read_csv('ecommerce_carbon_dataset.csv', parse_dates=['order_date'])\n",
        "\n",
        "@st.cache_resource\n",
        "def load_models():\n",
        "    # Load clustering model\n",
        "    scaler, kmeans = joblib.load('clustering_model.pkl')\n",
        "\n",
        "    # Load Prophet model\n",
        "    with open('prophet_model.json', 'r') as f:\n",
        "        prophet_model = model_from_json(f.read())\n",
        "\n",
        "    # Load LSTM components\n",
        "    lstm_scaler = joblib.load('lstm_scaler.pkl')\n",
        "\n",
        "    return {\n",
        "        'scaler': scaler,\n",
        "        'kmeans': kmeans,\n",
        "        'prophet': prophet_model,\n",
        "        'lstm_scaler': lstm_scaler\n",
        "    }\n",
        "\n",
        "df = load_data()\n",
        "models = load_models()\n",
        "\n",
        "# Sidebar controls\n",
        "with st.sidebar:\n",
        "    st.header(\"Filters\")\n",
        "    raw_date_range = st.date_input(\"Date Range\",\n",
        "        [df['order_date'].min(), df['order_date'].max()])\n",
        "\n",
        "    # Convert to pandas timestamps\n",
        "    date_range = [pd.Timestamp(d) for d in raw_date_range]\n",
        "\n",
        "    transport_modes = st.multiselect(\"Transport Modes\",\n",
        "        df['transport_mode'].unique())\n",
        "\n",
        "# Filter data with compatible types\n",
        "filtered_df = df[\n",
        "    (df['order_date'].between(*date_range)) &\n",
        "    (df['transport_mode'].isin(transport_modes))\n",
        "]\n",
        "\n",
        "# Filter data\n",
        "filtered_df = df[\n",
        "    (df['order_date'].between(*date_range)) &\n",
        "    (df['transport_mode'].isin(transport_modes))\n",
        "]\n",
        "\n",
        "# Dashboard sections\n",
        "tab1, tab2, tab3 = st.tabs([\"Analytics\", \"Forecasting\", \"Recommendations\"])\n",
        "\n",
        "with tab1:\n",
        "    col1, col2 = st.columns(2)\n",
        "    with col1:\n",
        "        st.subheader(\"Emission Distribution\")\n",
        "        fig = px.histogram(filtered_df, x='carbon_emission_kg', nbins=50)\n",
        "        st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "    with col2:\n",
        "        st.subheader(\"Transport Impact\")\n",
        "        transport_stats = filtered_df.groupby('transport_mode')['carbon_emission_kg'].mean()\n",
        "        st.bar_chart(transport_stats)\n",
        "\n",
        "with tab2:\n",
        "    model_type = st.radio(\"Select Model\", [\"Prophet\", \"LSTM\"], horizontal=True)\n",
        "\n",
        "    if model_type == \"Prophet\":\n",
        "        future = models['prophet'].make_future_dataframe(periods=365)\n",
        "        forecast = models['prophet'].predict(future)\n",
        "        fig = plot_plotly(models['prophet'], forecast)\n",
        "        st.plotly_chart(fig, use_container_width=True)\n",
        "    else:\n",
        "        scaled_data = models['lstm_scaler'].transform(filtered_df[['carbon_emission_kg']])\n",
        "        # Add LSTM prediction logic here\n",
        "\n",
        "with tab3:\n",
        "    st.subheader(\"Optimization Suggestions\")\n",
        "    if not filtered_df.empty:\n",
        "        sample_tx = filtered_df.sample(1).iloc[0].to_dict()\n",
        "        features = models['scaler'].transform([[sample_tx['distance_km'],\n",
        "                                              sample_tx['carbon_emission_kg'],\n",
        "                                              sample_tx['order_value_usd']]])\n",
        "        cluster = models['kmeans'].predict(features)[0]\n",
        "        recommendations = {\n",
        "            0: \"✅ Efficient transaction - maintain current practices\",\n",
        "            1: f\"🚚 Switch to rail (Current: {sample_tx['transport_mode']})\",\n",
        "            2: f\"📦 Use paper packaging (Current: {sample_tx['packaging_material']})\",\n",
        "            3: f\"🌿 Carbon offset for ${sample_tx['order_value_usd']} order\",\n",
        "            4: f\"📈 Optimize packaging for {sample_tx['distance_km']}km shipment\"\n",
        "        }\n",
        "        st.success(recommendations.get(cluster, \"No recommendation available\"))\n",
        "    else:\n",
        "        st.warning(\"No transactions match current filters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1u8pXYZiIBvk",
        "outputId": "588ca4da-f21d-4980-e57d-2187e9f27702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Set ngrok authtoken (replace with your token)\n",
        "ngrok.set_auth_token(\"2yYh6CEZctF7pKa67YPQ26URNPo_7Qd9a9U7PxNY3dAXX6E18\")\n",
        "\n",
        "# Start Streamlit\n",
        "!streamlit run app.py --server.port 8501 &>/dev/null &\n",
        "\n",
        "# Create tunnel\n",
        "# Pass the port number directly as the first argument\n",
        "public_url = ngrok.connect(8501) # Changed from port=8502 to 8501 based on streamlit server port\n",
        "print(f\"🌍 Dashboard URL: {public_url}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgKEpjnPK0pr",
        "outputId": "6c6e5a98-c9da-4110-9705-34dbf28e8b6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌍 Dashboard URL: NgrokTunnel: \"https://909b-34-143-128-250.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}