{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBF0RFtcOaJa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import logging\n",
        "import requests\n",
        "from datetime import datetime\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class CarbonDataCollector:\n",
        "    def __init__(self, gcp_api_key):\n",
        "        self.gcp_api_key = gcp_api_key\n",
        "        self.carbon_factors = {}\n",
        "\n",
        "    def fetch_real_time_carbon_factors(self):\n",
        "        try:\n",
        "            url = f\"https://sustainability.googleapis.com/v1/carbonFactors?key={self.gcp_api_key}\"\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            self.carbon_factors = response.json()\n",
        "            logging.info(\"Successfully fetched carbon factors from Google Cloud Sustainability API\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logging.error(f\"Failed to fetch carbon factors: {e}\")\n",
        "\n",
        "    def import_shipping_data(self, csv_path):\n",
        "        try:\n",
        "            df = pd.read_csv(csv_path)\n",
        "            logging.info(f\"Loaded shipping data from {csv_path}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading shipping data: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def import_energy_data(self, csv_path):\n",
        "        try:\n",
        "            df = pd.read_csv(csv_path)\n",
        "            logging.info(f\"Loaded warehouse energy data from {csv_path}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading energy data: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def import_supplier_data(self, csv_path):\n",
        "        try:\n",
        "            df = pd.read_csv(csv_path)\n",
        "            logging.info(f\"Loaded supplier carbon intensity data from {csv_path}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading supplier data: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "# Data Validation Functions\n",
        "def check_missing_values(df, critical_columns):\n",
        "    missing = df[critical_columns].isnull().sum()\n",
        "    for col, count in missing.items():\n",
        "        if count > 0:\n",
        "            logging.warning(f\"Missing values in column '{col}': {count}\")\n",
        "    return missing.sum() == 0\n",
        "\n",
        "def validate_emission_factors(df, factor_column, min_val=0, max_val=100):\n",
        "    invalid = df[(df[factor_column] < min_val) | (df[factor_column] > max_val)]\n",
        "    if not invalid.empty:\n",
        "        logging.warning(f\"Found out-of-range emission factors in column '{factor_column}'\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def ensure_data_consistency(df_list, key_column):\n",
        "    keys = [set(df[key_column].dropna()) for df in df_list if key_column in df.columns]\n",
        "    common_keys = set.intersection(*keys) if keys else set()\n",
        "    for df in df_list:\n",
        "        inconsistent = ~df[key_column].isin(common_keys)\n",
        "        if inconsistent.any():\n",
        "            logging.warning(f\"Inconsistencies found in {key_column} across datasets\")\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "# Preprocessing Pipeline\n",
        "def standardize_units(df, column, factor):\n",
        "    df[column] *= factor\n",
        "    logging.info(f\"Standardized units in column '{column}' using factor {factor}\")\n",
        "    return df\n",
        "\n",
        "def handle_date_formats(df, date_column):\n",
        "    try:\n",
        "        df[date_column] = pd.to_datetime(df[date_column], errors='coerce')\n",
        "        if df[date_column].isnull().any():\n",
        "            logging.warning(f\"Some dates could not be parsed in column '{date_column}'\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error converting date formats: {e}\")\n",
        "    return df\n",
        "\n",
        "def categorize_emission_scope(df, scope_column, mapping):\n",
        "    df['Emission Scope'] = df[scope_column].map(mapping)\n",
        "    if df['Emission Scope'].isnull().any():\n",
        "        logging.warning(\"Some emission scopes could not be mapped\")\n",
        "    return df\n",
        "\n",
        "# Example usage (would be in main script, not module)\n",
        "# collector = CarbonDataCollector(gcp_api_key=\"your-api-key\")\n",
        "# collector.fetch_real_time_carbon_factors()\n",
        "# shipping_df = collector.import_shipping_data(\"shipping.csv\")\n",
        "# energy_df = collector.import_energy_data(\"energy.csv\")\n",
        "# supplier_df = collector.import_supplier_data(\"suppliers.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "\n",
        "# Page config\n",
        "st.set_page_config(page_title=\"Carbon Data Uploader\", layout=\"wide\")\n",
        "st.title(\"üì¶ E-Commerce Carbon Footprint Data Uploader\")\n",
        "\n",
        "# Required columns dictionary\n",
        "REQUIRED_COLUMNS = {\n",
        "    \"Transportation\": [\"distance_km\", \"weight_kg\", \"transport_mode\"],\n",
        "    \"Warehouse\": [\"electricity_kwh\", \"gas_m3\", \"renewables_kwh\"],\n",
        "    \"Supplier\": [\"supplier_name\", \"carbon_intensity_kgco2e\"],\n",
        "    \"Packaging\": [\"material_type\", \"weight_kg\", \"emission_factor\"]\n",
        "}\n",
        "\n",
        "# File uploader section\n",
        "st.sidebar.header(\"Upload CSV Data Files\")\n",
        "\n",
        "uploaded_files = {\n",
        "    \"Transportation\": st.sidebar.file_uploader(\"Transportation Data\", type=\"csv\", key=\"transportation\"),\n",
        "    \"Warehouse\": st.sidebar.file_uploader(\"Warehouse Energy Data\", type=\"csv\", key=\"warehouse\"),\n",
        "    \"Supplier\": st.sidebar.file_uploader(\"Supplier Emissions Data\", type=\"csv\", key=\"supplier\"),\n",
        "    \"Packaging\": st.sidebar.file_uploader(\"Packaging Materials Data\", type=\"csv\", key=\"packaging\")\n",
        "}\n",
        "\n",
        "# Session state management for dataframes\n",
        "if \"dataframes\" not in st.session_state:\n",
        "    st.session_state[\"dataframes\"] = {}\n",
        "\n",
        "# Function: Data Preview\n",
        "def preview_data(df, label):\n",
        "    st.subheader(f\"üîç Preview: {label} Data\")\n",
        "    st.dataframe(df.head(10))\n",
        "    st.write(\"**Data Types:**\")\n",
        "    st.write(df.dtypes)\n",
        "    st.write(\"**Null Values:**\")\n",
        "    st.write(df.isnull().sum())\n",
        "    st.write(\"**Basic Statistics (numerical):**\")\n",
        "    st.write(df.describe())\n",
        "\n",
        "# Function: Data Validation\n",
        "def validate_data(df, required_cols):\n",
        "    results = []\n",
        "    for col in required_cols:\n",
        "        if col not in df.columns:\n",
        "            results.append((col, \"‚ùå Missing\"))\n",
        "        else:\n",
        "            results.append((col, \"‚úÖ Present\"))\n",
        "    st.write(\"### ‚úÖ Data Validation\")\n",
        "    for col, status in results:\n",
        "        st.write(f\"- `{col}`: {status}\")\n",
        "\n",
        "    suggestions = []\n",
        "    if df.isnull().sum().sum() > 0:\n",
        "        suggestions.append(\"Check for and handle missing values (e.g., imputation or removal).\")\n",
        "    for col in required_cols:\n",
        "        if col in df.columns and df[col].dtype == object:\n",
        "            try:\n",
        "                pd.to_numeric(df[col])\n",
        "            except ValueError:\n",
        "                suggestions.append(f\"Column `{col}` might contain non-numeric data.\")\n",
        "    if suggestions:\n",
        "        st.write(\"### ‚ö†Ô∏è Suggestions\")\n",
        "        for s in suggestions:\n",
        "            st.write(f\"- {s}\")\n",
        "\n",
        "# Main logic\n",
        "for label, uploaded_file in uploaded_files.items():\n",
        "    if uploaded_file is not None:\n",
        "        try:\n",
        "            df = pd.read_csv(uploaded_file)\n",
        "            st.session_state[\"dataframes\"][label] = df\n",
        "            preview_data(df, label)\n",
        "            validate_data(df, REQUIRED_COLUMNS[label])\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error reading {label} data: {e}\")\n",
        "\n",
        "# Final message\n",
        "if st.session_state[\"dataframes\"]:\n",
        "    st.success(\"‚úÖ Data loaded and validated. Ready for analysis or processing.\")\n"
      ],
      "metadata": {
        "id": "U421RQUlOklY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import requests\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "class EmissionFactorManager:\n",
        "    def __init__(self):\n",
        "        self.emission_factors = {\n",
        "            \"transportation\": {\n",
        "                \"truck\": 0.9,\n",
        "                \"van\": 0.2,\n",
        "                \"car\": 0.15\n",
        "            },\n",
        "            \"energy\": {\n",
        "                \"electricity\": 0.5,\n",
        "                \"natural_gas\": 0.2\n",
        "            },\n",
        "            \"packaging\": {\n",
        "                \"cardboard\": 0.2,\n",
        "                \"plastic\": 0.5\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def get_factor(self, category, item):\n",
        "        try:\n",
        "            factor = self.emission_factors[category][item]\n",
        "            logging.info(f\"Retrieved emission factor: {category}/{item} = {factor} kg CO2e\")\n",
        "            return factor\n",
        "        except KeyError:\n",
        "            logging.error(f\"Emission factor not found for {category}/{item}\")\n",
        "            return None\n",
        "\n",
        "    def update_from_api(self, source=\"DEFRA\"):\n",
        "        url_map = {\n",
        "            \"DEFRA\": \"https://api.defra.uk/emission-factors\",\n",
        "            \"EPA\": \"https://api.epa.gov/emission-factors\"\n",
        "        }\n",
        "        try:\n",
        "            url = url_map.get(source.upper())\n",
        "            if not url:\n",
        "                raise ValueError(\"Unsupported data source\")\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "            self._update_factors(data)\n",
        "            logging.info(f\"Emission factors updated from {source}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to update from {source}: {e}\")\n",
        "\n",
        "    def _update_factors(self, data):\n",
        "        for category in data:\n",
        "            if category not in self.emission_factors:\n",
        "                self.emission_factors[category] = {}\n",
        "            for item, value in data[category].items():\n",
        "                self.emission_factors[category][item] = value\n",
        "\n",
        "    def calculate_composite_factor(self, category, items_with_weights):\n",
        "        total_emission = 0\n",
        "        total_weight = 0\n",
        "        for item, weight in items_with_weights.items():\n",
        "            factor = self.get_factor(category, item)\n",
        "            if factor is not None:\n",
        "                total_emission += factor * weight\n",
        "                total_weight += weight\n",
        "        if total_weight == 0:\n",
        "            logging.warning(\"Total weight is zero, cannot calculate composite factor\")\n",
        "            return 0\n",
        "        composite = total_emission / total_weight\n",
        "        logging.info(f\"Composite emission factor for {category}: {composite:.3f} kg CO2e\")\n",
        "        return composite\n",
        "\n",
        "# Example usage:\n",
        "# manager = EmissionFactorManager()\n",
        "# factor = manager.get_factor(\"transportation\", \"van\")\n",
        "# manager.update_from_api(\"DEFRA\")\n",
        "# composite = manager.calculate_composite_factor(\"packaging\", {\"cardboard\": 2, \"plastic\": 1})\n"
      ],
      "metadata": {
        "id": "dVKqOGToOpT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "class CarbonEDA:\n",
        "    def __init__(self, emissions_df):\n",
        "        \"\"\"\n",
        "        emissions_df must contain at least:\n",
        "        - scope: 1, 2, or 3\n",
        "        - order_id or shipment_id\n",
        "        - emission_kg_co2e\n",
        "        - date\n",
        "        - origin, destination (optional)\n",
        "        - transport_mode (optional)\n",
        "        - distance_km, weight_kg (optional)\n",
        "        - supplier_location, carbon_intensity (optional)\n",
        "        - warehouse_id, energy_type, energy_consumed (optional)\n",
        "        \"\"\"\n",
        "        self.df = emissions_df.copy()\n",
        "        self.df['date'] = pd.to_datetime(self.df['date'], errors='coerce')\n",
        "\n",
        "    def summarize_emissions(self):\n",
        "        logging.info(\"Summarizing total emissions by scope\")\n",
        "        by_scope = self.df.groupby('scope')['emission_kg_co2e'].sum()\n",
        "        logging.info(f\"Emissions by Scope:\\n{by_scope}\")\n",
        "\n",
        "        logging.info(\"Calculating emissions per order\")\n",
        "        per_order = self.df.groupby('order_id')['emission_kg_co2e'].sum()\n",
        "\n",
        "        logging.info(\"Detecting seasonal patterns\")\n",
        "        self.df['month'] = self.df['date'].dt.month\n",
        "        monthly = self.df.groupby('month')['emission_kg_co2e'].sum()\n",
        "\n",
        "        logging.info(\"Geographic high-emission route detection\")\n",
        "        if 'origin' in self.df.columns and 'destination' in self.df.columns:\n",
        "            route_emissions = self.df.groupby(['origin', 'destination'])['emission_kg_co2e'].sum().sort_values(ascending=False).head(10)\n",
        "        else:\n",
        "            route_emissions = pd.Series()\n",
        "\n",
        "        return {\n",
        "            'by_scope': by_scope,\n",
        "            'per_order': per_order,\n",
        "            'monthly': monthly,\n",
        "            'route_emissions': route_emissions\n",
        "        }\n",
        "\n",
        "    def correlation_analysis(self):\n",
        "        corr_data = {}\n",
        "        if {'weight_kg', 'emission_kg_co2e'}.issubset(self.df.columns):\n",
        "            corr_data['weight_vs_emissions'] = self.df[['weight_kg', 'emission_kg_co2e']].corr().iloc[0,1]\n",
        "        if {'distance_km', 'emission_kg_co2e', 'transport_mode'}.issubset(self.df.columns):\n",
        "            mode_eff = self.df.groupby('transport_mode').apply(lambda x: (x['emission_kg_co2e']/x['distance_km']).mean())\n",
        "            corr_data['transport_efficiency'] = mode_eff\n",
        "        if {'supplier_location', 'carbon_intensity'}.issubset(self.df.columns):\n",
        "            supplier_intensity = self.df.groupby('supplier_location')['carbon_intensity'].mean().sort_values(ascending=False)\n",
        "            corr_data['supplier_intensity'] = supplier_intensity\n",
        "        return corr_data\n",
        "\n",
        "    def find_optimization_opportunities(self):\n",
        "        suggestions = []\n",
        "        if {'origin', 'destination', 'transport_mode'}.issubset(self.df.columns):\n",
        "            route_summary = self.df.groupby(['origin', 'destination', 'transport_mode'])['emission_kg_co2e'].sum().reset_index()\n",
        "            top_routes = route_summary.sort_values(by='emission_kg_co2e', ascending=False).head(5)\n",
        "            for _, row in top_routes.iterrows():\n",
        "                if row['transport_mode'] != 'rail':\n",
        "                    suggestions.append(f\"Route {row['origin']} ‚Üí {row['destination']} using {row['transport_mode']} emits {row['emission_kg_co2e']:.2f} kg CO2e. Consider switching to rail.\")\n",
        "\n",
        "        if {'material_type', 'weight_kg', 'emission_kg_co2e'}.issubset(self.df.columns):\n",
        "            packaging_avg = self.df.groupby('material_type').agg({\n",
        "                'weight_kg': 'mean', 'emission_kg_co2e': 'mean'\n",
        "            })\n",
        "            high_emitters = packaging_avg.sort_values('emission_kg_co2e', ascending=False).head(3)\n",
        "            for mat in high_emitters.index:\n",
        "                suggestions.append(f\"Material '{mat}' has high average emissions. Consider switching to a lower impact material.\")\n",
        "\n",
        "        if {'warehouse_id', 'energy_type', 'energy_consumed'}.issubset(self.df.columns):\n",
        "            warehouse_energy = self.df.groupby(['warehouse_id', 'energy_type'])['energy_consumed'].sum().unstack().fillna(0)\n",
        "            if 'renewable' in warehouse_energy.columns:\n",
        "                warehouse_energy['renewable_ratio'] = warehouse_energy['renewable'] / warehouse_energy.sum(axis=1)\n",
        "                low_renew = warehouse_energy[warehouse_energy['renewable_ratio'] < 0.3]\n",
        "                for warehouse in low_renew.index:\n",
        "                    suggestions.append(f\"Warehouse {warehouse} has low renewable energy use. Consider solar or wind integration.\")\n",
        "\n",
        "        return suggestions\n",
        "\n",
        "    def generate_automated_insights(self):\n",
        "        summary = self.summarize_emissions()\n",
        "        correlation = self.correlation_analysis()\n",
        "        optimization = self.find_optimization_opportunities()\n",
        "\n",
        "        insights = {\n",
        "            \"Summary\": summary,\n",
        "            \"Correlations\": correlation,\n",
        "            \"Recommendations\": optimization\n",
        "        }\n",
        "        return insights\n",
        "\n",
        "# Example usage:\n",
        "# df = pd.read_csv(\"emissions_dataset.csv\")\n",
        "# eda = CarbonEDA(df)\n",
        "# insights = eda.generate_automated_insights()\n",
        "# print(insights['Summary'])\n",
        "# print(insights['Correlations'])\n",
        "# print(insights['Recommendations'])\n"
      ],
      "metadata": {
        "id": "Jwo-XuSQOszF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from io import BytesIO\n",
        "from datetime import datetime\n",
        "\n",
        "# Load processed EDA class\n",
        "from eda_carbon_footprint import CarbonEDA\n",
        "\n",
        "st.set_page_config(layout=\"wide\")\n",
        "st.title(\"üìä E-Commerce Carbon Footprint EDA Dashboard\")\n",
        "\n",
        "# --- File Upload --- #\n",
        "uploaded_file = st.file_uploader(\"Upload emissions dataset (CSV)\", type=[\"csv\"])\n",
        "if uploaded_file:\n",
        "    df = pd.read_csv(uploaded_file, parse_dates=[\"date\"], dayfirst=True)\n",
        "    eda = CarbonEDA(df)\n",
        "\n",
        "    # --- Filters --- #\n",
        "    st.sidebar.header(\"üìå Filters\")\n",
        "    date_min, date_max = df['date'].min(), df['date'].max()\n",
        "    date_range = st.sidebar.date_input(\"Select Date Range\", [date_min, date_max])\n",
        "\n",
        "    df_filtered = df[(df['date'] >= pd.to_datetime(date_range[0])) & (df['date'] <= pd.to_datetime(date_range[1]))]\n",
        "\n",
        "    if 'region' in df.columns:\n",
        "        regions = st.sidebar.multiselect(\"Select Regions\", df['region'].dropna().unique(), default=df['region'].dropna().unique())\n",
        "        df_filtered = df_filtered[df_filtered['region'].isin(regions)]\n",
        "\n",
        "    if 'product_category' in df.columns:\n",
        "        products = st.sidebar.multiselect(\"Select Product Categories\", df['product_category'].dropna().unique(), default=df['product_category'].dropna().unique())\n",
        "        df_filtered = df_filtered[df_filtered['product_category'].isin(products)]\n",
        "\n",
        "    if 'transport_mode' in df.columns:\n",
        "        modes = st.sidebar.multiselect(\"Select Transport Modes\", df['transport_mode'].dropna().unique(), default=df['transport_mode'].dropna().unique())\n",
        "        df_filtered = df_filtered[df_filtered['transport_mode'].isin(modes)]\n",
        "\n",
        "    # --- Overview Metrics --- #\n",
        "    st.subheader(\"üìà Overview Metrics\")\n",
        "    scope_emissions = df_filtered.groupby('scope')['emission_kg_co2e'].sum()\n",
        "    cols = st.columns(3)\n",
        "    for i, scope in enumerate([1, 2, 3]):\n",
        "        cols[i].metric(label=f\"Scope {scope} Emissions (kg CO‚ÇÇe)\", value=round(scope_emissions.get(scope, 0), 2))\n",
        "\n",
        "    # Monthly trend\n",
        "    df_filtered['month'] = df_filtered['date'].dt.to_period(\"M\").astype(str)\n",
        "    month_trend = df_filtered.groupby('month')['emission_kg_co2e'].sum().reset_index()\n",
        "    fig1 = px.line(month_trend, x='month', y='emission_kg_co2e', title=\"üìÖ Monthly Emission Trend\", markers=True)\n",
        "    st.plotly_chart(fig1, use_container_width=True)\n",
        "\n",
        "    # Top 10 Emission Sources\n",
        "    if 'origin' in df.columns and 'destination' in df.columns:\n",
        "        route_emissions = df_filtered.groupby(['origin', 'destination'])['emission_kg_co2e'].sum().sort_values(ascending=False).head(10).reset_index()\n",
        "        route_emissions['route'] = route_emissions['origin'] + \" ‚Üí \" + route_emissions['destination']\n",
        "        fig2 = px.bar(route_emissions, x='route', y='emission_kg_co2e', title=\"üöõ Top 10 High-Emission Routes\")\n",
        "        st.plotly_chart(fig2, use_container_width=True)\n",
        "\n",
        "    # --- Visualizations --- #\n",
        "    st.subheader(\"üìä Visualizations\")\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    # Emission by Region (Heat Map)\n",
        "    if 'region' in df.columns:\n",
        "        with col1:\n",
        "            region_emissions = df_filtered.groupby('region')['emission_kg_co2e'].sum().reset_index()\n",
        "            fig3 = px.treemap(region_emissions, path=['region'], values='emission_kg_co2e', title=\"üó∫Ô∏è Emission Intensity by Region\")\n",
        "            st.plotly_chart(fig3, use_container_width=True)\n",
        "\n",
        "    # Correlation Scatter\n",
        "    with col2:\n",
        "        if {'distance_km', 'emission_kg_co2e'}.issubset(df_filtered.columns):\n",
        "            fig4 = px.scatter(df_filtered, x='distance_km', y='emission_kg_co2e', color='transport_mode', trendline='ols', title=\"üìç Distance vs Emissions\")\n",
        "            st.plotly_chart(fig4, use_container_width=True)\n",
        "\n",
        "    # Distribution of Emission Factors\n",
        "    if 'emission_kg_co2e' in df_filtered.columns:\n",
        "        st.subheader(\"üìà Emission Distribution\")\n",
        "        fig5 = px.histogram(df_filtered, x='emission_kg_co2e', nbins=50, title=\"Distribution of Emissions per Record\")\n",
        "        st.plotly_chart(fig5, use_container_width=True)\n",
        "\n",
        "    # --- Automated Insights Report --- #\n",
        "    st.subheader(\"üß† Automated Insights & Recommendations\")\n",
        "    insights = eda.generate_automated_insights()\n",
        "    st.write(\"### Summary\")\n",
        "    st.dataframe(insights['Summary']['by_scope'].reset_index(name='emissions (kg CO‚ÇÇe)'))\n",
        "    st.write(\"### Top Optimization Opportunities\")\n",
        "    for rec in insights['Recommendations']:\n",
        "        st.info(rec)\n",
        "\n",
        "    # Download Report\n",
        "    def generate_report():\n",
        "        buffer = BytesIO()\n",
        "        with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:\n",
        "            insights['Summary']['by_scope'].to_excel(writer, sheet_name='Scope Summary')\n",
        "            insights['Summary']['monthly'].to_excel(writer, sheet_name='Monthly Trends')\n",
        "            if not insights['Summary']['route_emissions'].empty:\n",
        "                insights['Summary']['route_emissions'].to_frame(name='kg CO‚ÇÇe').to_excel(writer, sheet_name='Top Routes')\n",
        "            pd.DataFrame(insights['Recommendations'], columns=[\"Recommendations\"]).to_excel(writer, sheet_name='Recommendations')\n",
        "        buffer.seek(0)\n",
        "        return buffer\n",
        "\n",
        "    st.download_button(\n",
        "        label=\"üì• Download Insights Report\",\n",
        "        data=generate_report(),\n",
        "        file_name=f\"carbon_eda_report_{datetime.now().strftime('%Y%m%d')}.xlsx\",\n",
        "        mime=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "t9bpbmLSOw0s",
        "outputId": "47f43974-25df-44dd-fbd6-95d14d4cf21a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'streamlit'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d90badf792ac>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstreamlit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'streamlit'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from prophet import Prophet\n",
        "from prophet.diagnostics import performance_metrics, cross_validation\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "class EmissionForecaster:\n",
        "    def __init__(self, df, date_col='date', emission_col='emission_kg_co2e', regressors=None):\n",
        "        \"\"\"\n",
        "        df: DataFrame with time series emissions data\n",
        "        regressors: list of additional column names to include as external regressors\n",
        "        \"\"\"\n",
        "        self.raw_df = df.copy()\n",
        "        self.date_col = date_col\n",
        "        self.emission_col = emission_col\n",
        "        self.regressors = regressors if regressors else []\n",
        "        self.model = None\n",
        "\n",
        "        # Prepare the data for Prophet\n",
        "        self.df = self.prepare_data()\n",
        "\n",
        "    def prepare_data(self):\n",
        "        df = self.raw_df[[self.date_col, self.emission_col] + self.regressors].copy()\n",
        "        df.rename(columns={self.date_col: 'ds', self.emission_col: 'y'}, inplace=True)\n",
        "        df['ds'] = pd.to_datetime(df['ds'])\n",
        "\n",
        "        # Fill missing dates\n",
        "        df = df.set_index('ds').asfreq('D')\n",
        "        df['y'].interpolate(method='linear', inplace=True)\n",
        "\n",
        "        for reg in self.regressors:\n",
        "            df[reg].fillna(method='ffill', inplace=True)\n",
        "\n",
        "        return df.reset_index()\n",
        "\n",
        "    def add_holidays(self):\n",
        "        # Example shipping holidays (can be expanded/customized)\n",
        "        return pd.DataFrame({\n",
        "            'holiday': 'peak_shipping',\n",
        "            'ds': pd.to_datetime([\n",
        "                '2022-11-25', '2022-12-15', '2023-11-24', '2023-12-15',\n",
        "                '2024-11-29', '2024-12-15'\n",
        "            ]),\n",
        "            'lower_window': -1,\n",
        "            'upper_window': 2\n",
        "        })\n",
        "\n",
        "    def train_model(self):\n",
        "        self.model = Prophet(\n",
        "            daily_seasonality=True,\n",
        "            weekly_seasonality=True,\n",
        "            yearly_seasonality=True,\n",
        "            holidays=self.add_holidays()\n",
        "        )\n",
        "\n",
        "        for reg in self.regressors:\n",
        "            self.model.add_regressor(reg)\n",
        "\n",
        "        self.model.fit(self.df)\n",
        "        logging.info(\"Prophet model trained successfully.\")\n",
        "\n",
        "    def forecast(self, months=6):\n",
        "        future = self.model.make_future_dataframe(periods=months*30)\n",
        "\n",
        "        # Add external regressors if needed\n",
        "        for reg in self.regressors:\n",
        "            last_val = self.df[reg].iloc[-1]\n",
        "            future[reg] = last_val  # simple constant extrapolation\n",
        "\n",
        "        forecast = self.model.predict(future)\n",
        "        return forecast\n",
        "\n",
        "    def plot_forecast(self, forecast):\n",
        "        fig1 = self.model.plot(forecast)\n",
        "        plt.title(\"Carbon Emissions Forecast\")\n",
        "        plt.show()\n",
        "\n",
        "    def detect_anomalies(self, forecast):\n",
        "        forecast['residual'] = forecast['yhat'] - forecast['yhat_lower']\n",
        "        anomalies = forecast[forecast['residual'] < -2 * forecast['residual'].std()]\n",
        "        return anomalies[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\n",
        "\n",
        "    def evaluate_model(self, horizon='90 days'):\n",
        "        df_cv = cross_validation(self.model, horizon=horizon)\n",
        "        df_p = performance_metrics(df_cv)\n",
        "        return df_p[['mae', 'mape', 'rmse']].mean()\n",
        "\n",
        "# Example Usage:\n",
        "# df = pd.read_csv('emissions_time_series.csv')\n",
        "# forecaster = EmissionForecaster(df, regressors=['economic_index', 'supply_disruption'])\n",
        "# forecaster.train_model()\n",
        "# forecast_df = forecaster.forecast(months=12)\n",
        "# print(forecaster.evaluate_model())\n",
        "# anomalies = forecaster.detect_anomalies(forecast_df)\n",
        "# forecaster.plot_forecast(forecast_df)\n"
      ],
      "metadata": {
        "id": "rib0zaKyO1Pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import shap\n",
        "\n",
        "class CarbonLSTMPredictor:\n",
        "    def __init__(self, sequence_length=30, feature_columns=None):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.feature_columns = feature_columns\n",
        "        self.model = None\n",
        "        self.scaler = MinMaxScaler()\n",
        "        self.history = None\n",
        "\n",
        "    def preprocess_data(self, df, target_column):\n",
        "        df = df.copy()\n",
        "        df.dropna(inplace=True)\n",
        "\n",
        "        if self.feature_columns is None:\n",
        "            self.feature_columns = [col for col in df.columns if col != target_column]\n",
        "\n",
        "        scaled = self.scaler.fit_transform(df[self.feature_columns + [target_column]])\n",
        "        X, y = [], []\n",
        "        for i in range(self.sequence_length, len(scaled)):\n",
        "            X.append(scaled[i - self.sequence_length:i, :-1])\n",
        "            y.append(scaled[i, -1])\n",
        "        X, y = np.array(X), np.array(y)\n",
        "        return X, y\n",
        "\n",
        "    def build_model(self, input_shape):\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(64, return_sequences=True, input_shape=input_shape))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(LSTM(32))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Dense(1))\n",
        "\n",
        "        def custom_loss(y_true, y_pred):\n",
        "            return tf.reduce_mean(tf.square(y_true - y_pred)) * 0.7 + tf.reduce_mean(tf.abs(y_true - y_pred)) * 0.3\n",
        "\n",
        "        model.compile(optimizer='adam', loss=custom_loss, metrics=['mae'])\n",
        "        self.model = model\n",
        "\n",
        "    def train(self, X, y, epochs=50, batch_size=32, val_split=0.2):\n",
        "        callbacks = [\n",
        "            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "            ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5, verbose=1),\n",
        "            ModelCheckpoint(\"best_lstm_model.h5\", save_best_only=True, monitor=\"val_loss\")\n",
        "        ]\n",
        "        self.history = self.model.fit(\n",
        "            X, y,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_split=val_split,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "    def predict(self, X):\n",
        "        preds = self.model.predict(X)\n",
        "        return preds\n",
        "\n",
        "    def evaluate(self, X, y_true):\n",
        "        y_pred = self.model.predict(X)\n",
        "        y_true = self.scaler.inverse_transform(\n",
        "            np.hstack([X[:, -1, :-1], y_true.reshape(-1, 1)])\n",
        "        )[:, -1]\n",
        "        y_pred = self.scaler.inverse_transform(\n",
        "            np.hstack([X[:, -1, :-1], y_pred])\n",
        "        )[:, -1]\n",
        "        mae = mean_absolute_error(y_true, y_pred)\n",
        "        rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "        return mae, rmse\n",
        "\n",
        "    def explain_feature_importance(self, X_sample):\n",
        "        explainer = shap.DeepExplainer(self.model, X_sample[:100])\n",
        "        shap_values = explainer.shap_values(X_sample[:100])\n",
        "        shap.summary_plot(shap_values[0], X_sample[:100], feature_names=self.feature_columns)\n",
        "\n",
        "# Example usage:\n",
        "# df = pd.read_csv(\"carbon_data.csv\")\n",
        "# lstm_model = CarbonLSTMPredictor(sequence_length=30, feature_columns=[\"temperature\", \"economic_index\"])\n",
        "# X, y = lstm_model.preprocess_data(df, target_column=\"emission_kg_co2e\")\n",
        "# lstm_model.build_model(X.shape[1:])\n",
        "# lstm_model.train(X, y)\n",
        "# predictions = lstm_model.predict(X[-10:])\n",
        "# mae, rmse = lstm_model.evaluate(X, y)\n",
        "# lstm_model.explain_feature_importance(X[-100:])\n"
      ],
      "metadata": {
        "id": "Y0GxbYAkPTLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "class SupplierCarbonOptimizer:\n",
        "    def __init__(self, df):\n",
        "        self.df = df.copy()\n",
        "        self.scaler = StandardScaler()\n",
        "        self.kmeans = None\n",
        "        self.features = [\n",
        "            'carbon_intensity_per_unit',\n",
        "            'transport_distance_km',\n",
        "            'transport_mode_score',\n",
        "            'renewable_energy_percent',\n",
        "            'packaging_sustainability_score'\n",
        "        ]\n",
        "\n",
        "    def preprocess(self):\n",
        "        self.df.dropna(subset=self.features, inplace=True)\n",
        "        self.X = self.scaler.fit_transform(self.df[self.features])\n",
        "\n",
        "    def determine_optimal_k(self, max_k=10):\n",
        "        distortions = []\n",
        "        for k in range(1, max_k + 1):\n",
        "            kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "            kmeans.fit(self.X)\n",
        "            distortions.append(kmeans.inertia_)\n",
        "\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.plot(range(1, max_k + 1), distortions, 'bo-')\n",
        "        plt.xlabel('Number of clusters (k)')\n",
        "        plt.ylabel('Inertia')\n",
        "        plt.title('Elbow Method for Optimal k')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    def fit_clusters(self, n_clusters):\n",
        "        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        self.df['cluster'] = self.kmeans.fit_predict(self.X)\n",
        "\n",
        "    def label_clusters(self):\n",
        "        centroids = self.kmeans.cluster_centers_\n",
        "        labels = []\n",
        "        for center in centroids:\n",
        "            if center[0] < 0 and center[1] < 0:  # low intensity, low distance\n",
        "                labels.append('Low Carbon Efficient')\n",
        "            elif center[0] > 0 and center[1] > 0:\n",
        "                labels.append('High Carbon Intensive')\n",
        "            else:\n",
        "                labels.append('Moderate Carbon Supplier')\n",
        "        self.df['cluster_label'] = self.df['cluster'].map(dict(enumerate(labels)))\n",
        "\n",
        "    def recommend_switch(self, supplier_id_col='supplier_id'):\n",
        "        recommendations = []\n",
        "        for idx, row in self.df.iterrows():\n",
        "            if row['cluster_label'] == 'High Carbon Intensive':\n",
        "                alternatives = self.df[\n",
        "                    (self.df['cluster_label'] == 'Low Carbon Efficient') &\n",
        "                    (self.df[supplier_id_col] != row[supplier_id_col])\n",
        "                ]\n",
        "                if not alternatives.empty:\n",
        "                    alt = alternatives.sample(1).iloc[0]\n",
        "                    reduction = row['carbon_intensity_per_unit'] - alt['carbon_intensity_per_unit']\n",
        "                    recommendations.append({\n",
        "                        'current_supplier': row[supplier_id_col],\n",
        "                        'suggested_supplier': alt[supplier_id_col],\n",
        "                        'potential_reduction_kg_co2e': reduction\n",
        "                    })\n",
        "        return pd.DataFrame(recommendations)\n",
        "\n",
        "    def plot_clusters(self):\n",
        "        pca_df = pd.DataFrame(self.X, columns=self.features)\n",
        "        pca_df['cluster'] = self.df['cluster']\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.scatterplot(\n",
        "            x=pca_df[self.features[0]],\n",
        "            y=pca_df[self.features[1]],\n",
        "            hue=pca_df['cluster'],\n",
        "            palette='Set2'\n",
        "        )\n",
        "        plt.title('Supplier Clusters (based on Carbon Optimization)')\n",
        "        plt.xlabel(self.features[0])\n",
        "        plt.ylabel(self.features[1])\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "# Example Usage:\n",
        "# df = pd.read_csv('supplier_emission_data.csv')\n",
        "# optimizer = SupplierCarbonOptimizer(df)\n",
        "# optimizer.preprocess()\n",
        "# optimizer.determine_optimal_k()\n",
        "# optimizer.fit_clusters(n_clusters=3)\n",
        "# optimizer.label_clusters()\n",
        "# recommendations = optimizer.recommend_switch()\n",
        "# optimizer.plot_clusters()\n",
        "# print(recommendations)\n"
      ],
      "metadata": {
        "id": "K6GuZjrWPqRE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}